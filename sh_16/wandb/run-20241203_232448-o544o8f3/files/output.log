/opt/conda/envs/suyun_unlog/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 481/481 [00:00<00:00, 2.54MB/s]
tokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25.0/25.0 [00:00<00:00, 170kB/s]
vocab.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 27.7MB/s]
merges.txt: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 32.7MB/s]
tokenizer.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 18.2MB/s]
model.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499M/499M [00:02<00:00, 241MB/s]
Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Generating train split: 87866 examples [00:11, 7822.50 examples/s] 
Generating train split: 4887 examples [00:00, 6533.40 examples/s]
Generating train split: 4934 examples [00:00, 7869.61 examples/s]
Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87866/87866 [00:00<00:00, 181312.60 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87866/87866 [07:19<00:00, 200.00 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4887/4887 [00:00<00:00, 381094.78 examples/s]
Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4887/4887 [00:24<00:00, 197.63 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4934/4934 [00:00<00:00, 389958.28 examples/s]
Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4934/4934 [00:25<00:00, 196.90 examples/s]
/home/ubuntu/storage/roberta_acc/lrqa/run_lrqa.py:165: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  9%|███████████████▍                                                                                                                                                    | 1545/16476 [1:06:46<10:53:06,  2.62s/it]
{'loss': 1.3887, 'grad_norm': 1.1810299158096313, 'learning_rate': 3.033980582524272e-06, 'epoch': 0.09}
                                                                                                                                                                                                                   
{'eval_loss': 1.3862388134002686, 'eval_accuracy': 0.292203813791275, 'eval_runtime': 122.9731, 'eval_samples_per_second': 39.74, 'eval_steps_per_second': 2.488, 'epoch': 0.09}
{'loss': 1.3879, 'grad_norm': 1.5669059753417969, 'learning_rate': 6.067961165048544e-06, 'epoch': 0.18}
{'eval_loss': 1.386085867881775, 'eval_accuracy': 0.3480663001537323, 'eval_runtime': 258.2325, 'eval_samples_per_second': 18.925, 'eval_steps_per_second': 1.185, 'epoch': 0.18}
{'loss': 1.387, 'grad_norm': 2.211730718612671, 'learning_rate': 9.101941747572816e-06, 'epoch': 0.27}
{'eval_loss': 1.386315107345581, 'eval_accuracy': 0.23061183094978333, 'eval_runtime': 181.4878, 'eval_samples_per_second': 26.927, 'eval_steps_per_second': 1.686, 'epoch': 0.27}
